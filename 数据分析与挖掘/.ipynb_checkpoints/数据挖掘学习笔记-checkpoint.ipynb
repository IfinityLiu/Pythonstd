{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 网络爬虫的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:02:48.472273Z",
     "start_time": "2019-05-31T08:02:48.457962Z"
    },
    "hidden": true
   },
   "source": [
    "* 通用爬虫，没有过滤的过程\n",
    "* 聚焦爬虫，拥有过滤的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "# 正则表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:10:55.062253Z",
     "start_time": "2019-05-31T08:10:55.054033Z"
    },
    "hidden": true
   },
   "source": [
    "* 什么是正则表达式\n",
    "* 原子\n",
    "* 元字符\n",
    "* 模式修正符\n",
    "* 贪婪模式与懒惰模式\n",
    "* 正则表达式函数\n",
    "* 常见正则实例\n",
    "* 简单的爬虫\n",
    "* 从网页中提取初qq群\n",
    "* 作业：提取出版社信息并写入文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:13:15.113661Z",
     "start_time": "2019-05-31T08:13:15.099555Z"
    },
    "hidden": true
   },
   "source": [
    "## 什么是正则表达式\n",
    "世界上信息非常多，而我们关注的信息有限。加入我们希望只提取初关注的数据，此时可以通过一些表达式进行提取，正则表达式就是其中一种进行数据筛选的表达式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 原子\n",
    "原子是正则表达式中最基本的组成单位，每个正则表达式中至少要包含一个原子。常见的原子类型有：\n",
    "   * 普通字符作为原子\n",
    "   * 非打印字符作为原子\n",
    "   * 通用字符作为原子\n",
    "   * 原子表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> 接下来进行实战讲解\n",
    "https://www.bilibili.com/video/av22571713/?p=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.917365Z",
     "start_time": "2019-06-05T07:56:48.914020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T11:10:40.187162Z",
     "start_time": "2019-05-31T11:10:40.182941Z"
    },
    "hidden": true
   },
   "source": [
    "### 普通字符作为原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.928932Z",
     "start_time": "2019-06-05T07:56:48.920319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pat = \"yue\"\n",
    "string = \"http://yum.iqianyue.com\"\n",
    "rst1 = re.search(pat, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.943103Z",
     "start_time": "2019-06-05T07:56:48.931902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(16, 19), match='yue'>\n"
     ]
    }
   ],
   "source": [
    "print(rst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.960702Z",
     "start_time": "2019-06-05T07:56:48.945702Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "string2 = \"abcssds\"\n",
    "rst2 = re.search(pat, string2)\n",
    "print(rst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 非打印字符作为原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.973886Z",
     "start_time": "2019-06-05T07:56:48.963555Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(12, 13), match='\\n'>\n"
     ]
    }
   ],
   "source": [
    "pat2 = '\\n'\n",
    "string3 = \"\"\"jkhesljklkjl\n",
    "lkjlkjlkjlj\"\"\"\n",
    "rst3 = re.search(pat2, string3)\n",
    "print(rst3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 通用字符作为原子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "   * \\w 表示通用字符，匹配任意的字母、数字或者下划线；\n",
    "   * \\d 表示十进制数，匹配任意一个十进制数；\n",
    "   * \\s 表示空白字符，匹配任意一个空白字符；\n",
    "   * \\W 表示匹配与w相反的东西，即匹配除任意一个字母、数字或下划线的任意一个字符；\n",
    "   * \\D 表示匹配除十进制以外的任意一个字符；\n",
    "   * \\s 除了空白字符意外的任意一个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:48.989958Z",
     "start_time": "2019-06-05T07:56:48.979259Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pat3 = '\\w\\dpython\\w'\n",
    "string4 = 'sldjflsdjflsjkf56464654646python_adf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.006767Z",
     "start_time": "2019-06-05T07:56:48.993700Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(24, 33), match='46python_'>\n"
     ]
    }
   ],
   "source": [
    "rst4 = re.search(pat3, string4)\n",
    "print(rst4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 原子表\n",
    "> 原子表通常用[]括起来,定义一组地位平等的原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.017885Z",
     "start_time": "2019-06-05T07:56:49.009264Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pat4 = 'pyth[jsz]n'\n",
    "string1 = 'sjdlfjslkdjflksjfpythjnsss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.029891Z",
     "start_time": "2019-06-05T07:56:49.020791Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(17, 23), match='pythjn'>\n"
     ]
    }
   ],
   "source": [
    "rst5 = re.search(pat4, string1)\n",
    "print(rst5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.041755Z",
     "start_time": "2019-06-05T07:56:49.032382Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "string1 = 'sjdlfjslkdjflksjfpythjsnsss'\n",
    "rst5 = re.search(pat4, string1)\n",
    "print(rst5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T09:39:06.390392Z",
     "start_time": "2019-06-02T09:39:06.385131Z"
    },
    "hidden": true
   },
   "source": [
    "## 元字符\n",
    "> 所谓元字符，就是正则表达式中具有一些特殊含义的字符，比如重复N次前面的字符等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "   * ’ . ‘ 能够匹配任意的字符；\n",
    "   * ’ ^ ‘ 匹配待搜索字符串的开始位置；\n",
    "   * ‘ $ ’ 主要用来匹配字符串结束的位置；\n",
    "   * ‘ * ’ 匹配0次、1次或者多次前面的原子；\n",
    "   * ‘ ? ’ 匹配0次或者1次前面的原子；\n",
    "   * ‘ + ’ 匹配1次或者多次前面的原子；\n",
    "   * ‘ {数字} ’ 代表前面的原子出现了’数字‘次，数字后面跟‘ , ’表示至少出现‘数字’次\n",
    "   * ‘ {数字1，数字2} ’ 表示原子至少出现了‘数字1’次，最多出现‘数字2’次；\n",
    "   * ‘ | ’ 模式选择符、表示‘或’\n",
    "   * ‘ （） ’ 模式单元，通常用于提取某一个内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.053530Z",
     "start_time": "2019-06-05T07:56:49.044547Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 21), match='fpythonlsd'>\n"
     ]
    }
   ],
   "source": [
    "pat = \".python...\"\n",
    "string = 'sldkfjslddjfpythonlsdkjflsjkdf'\n",
    "rst = re.search(pat, string)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.067866Z",
     "start_time": "2019-06-05T07:56:49.056256Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 7), match='php'>\n"
     ]
    }
   ],
   "source": [
    "pat = 'python|php'\n",
    "string = 'abcdphpt5646pythonlkjls'\n",
    "rst = re.search(pat, string)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 模式修正符\n",
    "> 所谓模式修正符，即可以在不改变正则表达式的情况下，通过模式修正符改变正则表达式的含义，从而实现一些匹配结果的调整等功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "   1. ”I“：匹配时忽略大小写；\n",
    "   2. ”M“：多行匹配；\n",
    "   3. ”L“：本地化识别匹配；\n",
    "   4. ”U“：根据Unicode字符解析字符；\n",
    "   5. ”S“：让‘ . ’匹配换行符；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.081670Z",
     "start_time": "2019-06-05T07:56:49.070652Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "pat1 = 'python'\n",
    "pat2 = 'python'\n",
    "string = 'lsdfjklsjdfklsjPythonsldkfjls'\n",
    "rst1 = re.search(pat1, string)\n",
    "print(rst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.089661Z",
     "start_time": "2019-06-05T07:56:49.084172Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(15, 21), match='Python'>\n"
     ]
    }
   ],
   "source": [
    "rst2 = re.search(pat2, string, re.I)\n",
    "print(rst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 贪婪模式与懒惰模式\n",
    "> 贪婪模式的核心点就是尽可能多的匹配，而懒惰模式的核心点就是尽可能少的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.101250Z",
     "start_time": "2019-06-05T07:56:49.092353Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 28), match='pythonsdfsdfpy'>\n"
     ]
    }
   ],
   "source": [
    "pat1 = 'p.*y'  # 贪婪模式\n",
    "pat2 = 'p.*?y'  # 懒惰模式\n",
    "string1 = 'abcdsldkjflsdjpythonsdfsdfpy'\n",
    "rst1 = re.search(pat1, string1)\n",
    "print(rst1)\n",
    "# 尽可能多的匹配结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.111718Z",
     "start_time": "2019-06-05T07:56:49.103627Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 16), match='py'>\n"
     ]
    }
   ],
   "source": [
    "rst2 = re.search(pat2, string1)\n",
    "print(rst2)\n",
    "# 更精确的定位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 正则表达式函数\n",
    "> 正则表示函数有re.match()函数、re.search()函数、全局匹配函数、re.sub()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.124439Z",
     "start_time": "2019-06-05T07:56:49.114527Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "rst=re.match(pat1,string)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.133562Z",
     "start_time": "2019-06-05T07:56:49.126798Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 16), match='phsldkjflskjdfly'>\n"
     ]
    }
   ],
   "source": [
    "string = 'phsldkjflskjdflysdlfkj'\n",
    "rst = re.match(pat1, string)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.157713Z",
     "start_time": "2019-06-05T07:56:49.136640Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phsldkjflskjdfly']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全局匹配函数\n",
    "re.compile(pat1).findall(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 常见正则实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:49.167602Z",
     "start_time": "2019-06-05T07:56:49.160806Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.baidu.com']\n"
     ]
    }
   ],
   "source": [
    "# 匹配.com或.cn网址\n",
    "\n",
    "pat = '[a-zA-Z]+://[^\\s]*[.com|.cn]'\n",
    "string = '<a href=\"http://www.baidu.com\">sjdlfjsl</a>'\n",
    "\n",
    "rst = re.compile(pat).findall(string)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 简单的爬虫\n",
    "> 如何爬去CSDN学院，并提取出qq群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:50.595896Z",
     "start_time": "2019-06-05T07:56:49.174604Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "pat = '\"400-(.*?)\"'\n",
    "data = urllib.request.urlopen(\n",
    "    'https://edu.csdn.net/course/detail/24693').read()\n",
    "rst = re.compile(pat).findall(str(data))\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 作业\n",
    "> 将 https://read.douban.com/provider/all 中所有的出版社提取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:51.777110Z",
     "start_time": "2019-06-05T07:56:50.601258Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pat = '<div class=\"name\">(.*?)</div>'\n",
    "data = urllib.request.urlopen('https://read.douban.com/provider/all').read()\n",
    "data = data.decode(encoding='utf-8')\n",
    "rst = re.compile(pat).findall(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Urllib实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:52.877567Z",
     "start_time": "2019-06-05T07:56:51.780374Z"
    }
   },
   "outputs": [],
   "source": [
    "pat = '<div class=\"name\">(.*?)</div>'\n",
    "data = urllib.request.urlopen('https://read.douban.com/provider/all').read()\n",
    "data = data.decode(encoding='utf-8')\n",
    "rst = re.compile(pat).findall(str(data))\n",
    "file = open('abc.txt','w')\n",
    "for i in range(len(rst)):\n",
    "    file.write(rst[i]+'\\n')\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib基础\n",
    "   * **urlretrieve()**\n",
    "   * **urlcleanup()**\n",
    "   * **info()**\n",
    "   * **getcode()**\n",
    "   * **geturl()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:53.950543Z",
     "start_time": "2019-06-05T07:56:52.880465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/aaron/Documents/pyobj/数据分析与挖掘/1.html',\n",
       " <http.client.HTTPMessage at 0x7f92c31f4d30>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'http://www.hellobi.com'\n",
    "urllib.request.urlretrieve(\n",
    "    url, filename='/home/aaron/Documents/pyobj/数据分析与挖掘/1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:53.957548Z",
     "start_time": "2019-06-05T07:56:53.953633Z"
    }
   },
   "outputs": [],
   "source": [
    "# 清楚缓存 -> urlcleanup()\n",
    "\n",
    "urllib.request.urlcleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:54.804674Z",
     "start_time": "2019-06-05T07:56:53.961024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x7f92d806eda0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# info() -> 展现一些环境信息\n",
    "\n",
    "file = urllib.request.urlopen(url)\n",
    "file.info()\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:54.813107Z",
     "start_time": "2019-06-05T07:56:54.807469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当前网页状态码 -> getcode()\n",
    "\n",
    "file.getcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:54.828926Z",
     "start_time": "2019-06-05T07:56:54.816523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.hellobi.com/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当前爬取网页的地址 -> geturl()\n",
    "\n",
    "file.geturl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超时设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于网络速度或对方服务器的问题，在爬取一个网页的时候都需要时间。访问一个网页，如果该网页长时间未响应，系统就会判断该网页超时，即无法打开该网页。  \n",
    "有时需要根据自己的需要，来设置超时的时间值，比如，有些网站反应快，在设置时希望2秒无响应则判断为超时（timeout=2）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:56:55.627176Z",
     "start_time": "2019-06-05T07:56:54.832213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x7f92c323ecc0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.hellobi.com'\n",
    "urllib.request.urlopen(url, timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:01.134066Z",
     "start_time": "2019-06-05T07:56:55.629535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n",
      "14165\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    try:\n",
    "        file = urllib.request.urlopen('http://yum.iqianyue.com', timeout=1)\n",
    "        data = file.read()\n",
    "        print(len(data))\n",
    "    except Exception as e:\n",
    "        print('网址出现异常：'+str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动模拟HTTP请求  ！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端如果要与服务器端进行通信，需要通过http请求进行，http请求有很多种，常用一般为get与post两种。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:01.139946Z",
     "start_time": "2019-06-05T07:57:01.136435Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:02.383499Z",
     "start_time": "2019-06-05T07:57:01.143202Z"
    }
   },
   "outputs": [],
   "source": [
    "keywd = 'python'\n",
    "\n",
    "# 如果时中文的时要做如下处理：\n",
    "# keywd = urllib.request.quote(keywd)\n",
    "\n",
    "url = 'http://www.baidu.com/s?wd='+keywd\n",
    "\n",
    "req = urllib.request.Request(url)\n",
    "\n",
    "data = urllib.request.urlopen(req).read()\n",
    "\n",
    "fh = open('2.html', 'wb')\n",
    "fh.write(data)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:14.952914Z",
     "start_time": "2019-06-05T07:57:02.385695Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理post请求\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = 'https://www.iqianyue.com/mypost'\n",
    "mydata = urllib.parse.urlencode({\n",
    "    'name': 'abc',\n",
    "    'pass': 'abcd123'\n",
    "}).encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(url, mydata)\n",
    "data = urllib.request.urlopen(req).read()\n",
    "\n",
    "fh = open('3.html', 'wb')\n",
    "fh.write(data)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 爬虫的异常处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. **异常处理概述**\n",
    "   2. **常见状态码及含义**\n",
    "   3. **URLError与HTTPError**\n",
    "   4. **异常处理实战**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常处理概述\n",
    "> 爬虫在运行的过程中，很多时候都会遇到异常。如果没有异常处理，爬虫遇到异常时就会直接崩溃停止运行，下次再次运行时，又会重新开始，所以，要开发一个具有顽强生命力的爬虫，必须要进行异常处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见状态码及含义  \n",
    "   * **301 Moved Permanently**：*重新定向到新的URL，永久性*；\n",
    "   * **302 Found：重新定向到临时的URL**，*非永久性*；\n",
    "   * **304 Not Modified**：*请求的资源未更新*；\n",
    "   * **400 Bad Request**：*非法请求*；\n",
    "   * **401 Unauthorized**：*请求未经授权*；\n",
    "   * **403 Forbidden**：*禁止访问*；\n",
    "   * **404 Not Found**：*没有找到对应页面*；\n",
    "   * **500 Internal Server Error**：*服务器内部出现错误*；\n",
    "   * **501 Not Implemented**：*服务器不支持实现请求所需要的功能*；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLError与HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 两者都是异常处理的类，HTTPError时URLError的子类，HTTPError有异常状态码与异常原因，URLError没有异常状态码，所以，在处理的时候不能使用URLError直接代替HTTPError。如果要替代，必须要判断是否有状态码属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常处理实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLError产生的情况：\n",
    "   1. 连不上服务器\n",
    "   2. 远程URL不存在\n",
    "   3. 本地没有网络\n",
    "   4. 触发了对应的HTTPError子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:15.688614Z",
     "start_time": "2019-06-05T07:57:14.954742Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    urllib.request.urlopen('http://blog.csdn.net')\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e, 'code'):  # 因为URLError没有状态码，所以需要在此时进行一定的处理，处理后就能够代替HTTPError。\n",
    "        print(e.code)\n",
    "    if hasattr(e, '%reason'):\n",
    "        print(e.reason)\n",
    "\n",
    "# except的写法是固定的，可以复用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫的浏览器伪装技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. **浏览器伪装技术原理**\n",
    "   2. **浏览器伪装技术实战**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 浏览器伪装技术原理\n",
    "> 浏览器伪装一般通过报头进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:57:16.950366Z",
     "start_time": "2019-06-05T07:57:15.690899Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as ure\n",
    "\n",
    "url = 'https://blog.csdn.net/weiwei_pig/article/details/52123738'\n",
    "\n",
    "# 伪装浏览器->设置报头信息\n",
    "hearders = ('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36')\n",
    "\n",
    "opener = ure.build_opener()\n",
    "opener.addheaders = [hearders]\n",
    "data = opener.open(url).read()\n",
    "\n",
    "fh = open('4.html', 'wb')\n",
    "fh.write(data)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 浏览器伪装技术实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于urlopen()对于一些HTTP的高级功能不支持，所以，要修改报头，可以使用urllib.request.build_opener()进行，也可以使用urllib.request.Request()下的add_header()实现浏览器的模拟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新闻爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. **新闻爬虫需求实现思路**\n",
    "   2. **新闻爬虫编写实战**\n",
    "   3. **作业**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:51:47.778467Z",
     "start_time": "2019-06-05T07:51:47.774577Z"
    }
   },
   "source": [
    "### 新闻爬虫需求及实现思路\n",
    "> 需求：将新浪新闻首页（http://news.sina.com.cn/） 所有新闻都爬到本地  \n",
    "> 思路：先爬首页，通过正则获取所有新闻链接，然后依次爬各新闻，并存储到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T11:56:50.360923Z",
     "start_time": "2019-06-05T11:56:32.590653Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次爬取\n",
      "------爬取成功！-----\n",
      "第1次爬取\n",
      "------爬取成功！-----\n",
      "第2次爬取\n",
      "------爬取成功！-----\n",
      "第3次爬取\n",
      "------爬取成功！-----\n",
      "第4次爬取\n",
      "------爬取成功！-----\n",
      "第5次爬取\n",
      "------爬取成功！-----\n",
      "第6次爬取\n",
      "------爬取成功！-----\n",
      "第7次爬取\n",
      "------爬取成功！-----\n",
      "第8次爬取\n",
      "------爬取成功！-----\n",
      "第9次爬取\n",
      "------爬取成功！-----\n",
      "第10次爬取\n",
      "------爬取成功！-----\n",
      "第11次爬取\n",
      "------爬取成功！-----\n",
      "第12次爬取\n",
      "------爬取成功！-----\n",
      "第13次爬取\n",
      "------爬取成功！-----\n",
      "第14次爬取\n",
      "------爬取成功！-----\n",
      "第15次爬取\n",
      "------爬取成功！-----\n",
      "第16次爬取\n",
      "------爬取成功！-----\n",
      "第17次爬取\n",
      "------爬取成功！-----\n",
      "第18次爬取\n",
      "------爬取成功！-----\n",
      "第19次爬取\n",
      "------爬取成功！-----\n",
      "第20次爬取\n",
      "------爬取成功！-----\n",
      "第21次爬取\n",
      "------爬取成功！-----\n",
      "第22次爬取\n",
      "------爬取成功！-----\n",
      "第23次爬取\n",
      "------爬取成功！-----\n",
      "第24次爬取\n",
      "------爬取成功！-----\n",
      "第25次爬取\n",
      "------爬取成功！-----\n",
      "第26次爬取\n",
      "------爬取成功！-----\n",
      "第27次爬取\n",
      "------爬取成功！-----\n",
      "第28次爬取\n",
      "------爬取成功！-----\n",
      "第29次爬取\n",
      "------爬取成功！-----\n",
      "第30次爬取\n",
      "------爬取成功！-----\n"
     ]
    }
   ],
   "source": [
    "# 2019.06.05 16:13\n",
    "\n",
    "import urllib.request as ure\n",
    "import urllib.error\n",
    "import re\n",
    "\n",
    "url = 'http://news.sina.com.cn/'\n",
    "data = ure.urlopen(url).read()\n",
    "data1 = data.decode('utf-8')  # 如果出现编码错误可以增加一个参宿 -> 'ignore'\n",
    "pat = 'href=\"(http://slide.news.sina.com.cn/.*?)\"'  # 设置正则表达式\n",
    "all_url = re.compile(pat).findall(data1)\n",
    "for i in range(len(all_url)):\n",
    "    try:\n",
    "        print('第'+str(i)+'次爬取')\n",
    "        this_url = all_url[i]\n",
    "        file = './sinanews/'+str(i)+'.html'\n",
    "        ure.urlretrieve(this_url, file)\n",
    "        print('------爬取成功！-----')\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, 'code'):  # 因为URLError没有状态码，所以需要在此时进行一定的处理，处理后就能够代替HTTPError。\n",
    "            print(e.code)\n",
    "        if hasattr(e, '%reason'):\n",
    "            print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业\n",
    "爬取CSDN博客http://blog.csdn.net/　首页显示的所有文章，每个文章内容单独生产一个本地网页存到本地中。\n",
    "> 难点：浏览器伪装，循环爬各文章  \n",
    "> 思路：先爬首页，然后通过正则筛选出所有文章url，然后通过循环分别爬去这些url到本地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/av22571713/?p=21 看到此处！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:06:34.294158Z",
     "start_time": "2019-06-05T11:57:32.320366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次爬取\n",
      "------爬取成功！-----\n",
      "第1次爬取\n",
      "------爬取成功！-----\n",
      "第2次爬取\n",
      "------爬取成功！-----\n",
      "第3次爬取\n",
      "------爬取成功！-----\n",
      "第4次爬取\n",
      "------爬取成功！-----\n",
      "第5次爬取\n",
      "------爬取成功！-----\n",
      "第6次爬取\n",
      "------爬取成功！-----\n",
      "第7次爬取\n",
      "------爬取成功！-----\n",
      "第8次爬取\n",
      "------爬取成功！-----\n",
      "第9次爬取\n",
      "------爬取成功！-----\n",
      "第10次爬取\n",
      "------爬取成功！-----\n",
      "第11次爬取\n",
      "------爬取成功！-----\n",
      "第12次爬取\n",
      "------爬取成功！-----\n",
      "第13次爬取\n",
      "------爬取成功！-----\n",
      "第14次爬取\n",
      "------爬取成功！-----\n",
      "第15次爬取\n",
      "------爬取成功！-----\n",
      "第16次爬取\n",
      "------爬取成功！-----\n",
      "第17次爬取\n",
      "------爬取成功！-----\n",
      "第18次爬取\n",
      "------爬取成功！-----\n",
      "第19次爬取\n",
      "------爬取成功！-----\n",
      "第20次爬取\n",
      "------爬取成功！-----\n",
      "第21次爬取\n",
      "------爬取成功！-----\n",
      "第22次爬取\n",
      "------爬取成功！-----\n",
      "第23次爬取\n",
      "------爬取成功！-----\n",
      "第24次爬取\n",
      "------爬取成功！-----\n",
      "第25次爬取\n",
      "------爬取成功！-----\n",
      "第26次爬取\n",
      "------爬取成功！-----\n",
      "第27次爬取\n",
      "------爬取成功！-----\n",
      "第28次爬取\n",
      "------爬取成功！-----\n",
      "第29次爬取\n",
      "------爬取成功！-----\n",
      "第30次爬取\n",
      "------爬取成功！-----\n",
      "第31次爬取\n",
      "------爬取成功！-----\n",
      "第32次爬取\n",
      "------爬取成功！-----\n",
      "第33次爬取\n",
      "------爬取成功！-----\n",
      "第34次爬取\n",
      "------爬取成功！-----\n",
      "第35次爬取\n",
      "------爬取成功！-----\n",
      "第36次爬取\n",
      "------爬取成功！-----\n",
      "第37次爬取\n",
      "------爬取成功！-----\n",
      "第38次爬取\n",
      "------爬取成功！-----\n",
      "第39次爬取\n",
      "------爬取成功！-----\n",
      "第40次爬取\n",
      "------爬取成功！-----\n",
      "第41次爬取\n",
      "------爬取成功！-----\n",
      "第42次爬取\n",
      "------爬取成功！-----\n",
      "第43次爬取\n",
      "------爬取成功！-----\n",
      "第44次爬取\n",
      "------爬取成功！-----\n",
      "第45次爬取\n",
      "------爬取成功！-----\n",
      "第46次爬取\n",
      "------爬取成功！-----\n",
      "第47次爬取\n",
      "------爬取成功！-----\n",
      "第48次爬取\n",
      "------爬取成功！-----\n",
      "第49次爬取\n",
      "------爬取成功！-----\n",
      "第50次爬取\n",
      "------爬取成功！-----\n",
      "第51次爬取\n",
      "------爬取成功！-----\n",
      "第52次爬取\n",
      "------爬取成功！-----\n",
      "第53次爬取\n",
      "------爬取成功！-----\n",
      "第54次爬取\n",
      "------爬取成功！-----\n",
      "第55次爬取\n",
      "------爬取成功！-----\n",
      "第56次爬取\n",
      "------爬取成功！-----\n",
      "第57次爬取\n",
      "------爬取成功！-----\n",
      "第58次爬取\n",
      "------爬取成功！-----\n",
      "第59次爬取\n",
      "------爬取成功！-----\n",
      "第60次爬取\n",
      "------爬取成功！-----\n",
      "第61次爬取\n",
      "------爬取成功！-----\n",
      "第62次爬取\n",
      "------爬取成功！-----\n",
      "第63次爬取\n",
      "------爬取成功！-----\n",
      "第64次爬取\n",
      "------爬取成功！-----\n",
      "第65次爬取\n",
      "------爬取成功！-----\n",
      "第66次爬取\n",
      "------爬取成功！-----\n",
      "第67次爬取\n",
      "------爬取成功！-----\n",
      "第68次爬取\n",
      "------爬取成功！-----\n",
      "第69次爬取\n",
      "------爬取成功！-----\n",
      "第70次爬取\n",
      "------爬取成功！-----\n",
      "第71次爬取\n",
      "------爬取成功！-----\n",
      "第72次爬取\n",
      "------爬取成功！-----\n",
      "第73次爬取\n",
      "------爬取成功！-----\n",
      "第74次爬取\n",
      "------爬取成功！-----\n",
      "第75次爬取\n",
      "------爬取成功！-----\n",
      "第76次爬取\n",
      "------爬取成功！-----\n",
      "第77次爬取\n",
      "------爬取成功！-----\n",
      "第78次爬取\n",
      "------爬取成功！-----\n",
      "第79次爬取\n",
      "------爬取成功！-----\n",
      "第80次爬取\n",
      "------爬取成功！-----\n",
      "第81次爬取\n",
      "------爬取成功！-----\n",
      "第82次爬取\n",
      "------爬取成功！-----\n",
      "第83次爬取\n",
      "------爬取成功！-----\n",
      "第84次爬取\n",
      "------爬取成功！-----\n",
      "第85次爬取\n",
      "------爬取成功！-----\n",
      "第86次爬取\n",
      "------爬取成功！-----\n",
      "第87次爬取\n",
      "------爬取成功！-----\n",
      "第88次爬取\n",
      "------爬取成功！-----\n",
      "第89次爬取\n",
      "------爬取成功！-----\n",
      "第90次爬取\n",
      "------爬取成功！-----\n",
      "第91次爬取\n",
      "------爬取成功！-----\n",
      "第92次爬取\n",
      "------爬取成功！-----\n",
      "第93次爬取\n",
      "------爬取成功！-----\n",
      "第94次爬取\n",
      "------爬取成功！-----\n",
      "第95次爬取\n",
      "------爬取成功！-----\n",
      "第96次爬取\n",
      "------爬取成功！-----\n",
      "第97次爬取\n",
      "------爬取成功！-----\n",
      "第98次爬取\n",
      "------爬取成功！-----\n",
      "第99次爬取\n",
      "------爬取成功！-----\n",
      "第100次爬取\n",
      "------爬取成功！-----\n",
      "第101次爬取\n",
      "------爬取成功！-----\n",
      "第102次爬取\n",
      "------爬取成功！-----\n",
      "第103次爬取\n",
      "------爬取成功！-----\n",
      "第104次爬取\n",
      "------爬取成功！-----\n",
      "第105次爬取\n",
      "------爬取成功！-----\n",
      "第106次爬取\n",
      "------爬取成功！-----\n",
      "第107次爬取\n",
      "------爬取成功！-----\n",
      "第108次爬取\n",
      "------爬取成功！-----\n",
      "第109次爬取\n",
      "------爬取成功！-----\n",
      "第110次爬取\n",
      "------爬取成功！-----\n",
      "第111次爬取\n",
      "------爬取成功！-----\n",
      "第112次爬取\n",
      "------爬取成功！-----\n",
      "第113次爬取\n",
      "------爬取成功！-----\n",
      "第114次爬取\n",
      "------爬取成功！-----\n",
      "第115次爬取\n",
      "------爬取成功！-----\n",
      "第116次爬取\n",
      "------爬取成功！-----\n",
      "第117次爬取\n",
      "------爬取成功！-----\n",
      "第118次爬取\n",
      "------爬取成功！-----\n",
      "第119次爬取\n",
      "------爬取成功！-----\n",
      "第120次爬取\n",
      "------爬取成功！-----\n",
      "第121次爬取\n",
      "------爬取成功！-----\n",
      "第122次爬取\n",
      "------爬取成功！-----\n",
      "第123次爬取\n",
      "------爬取成功！-----\n",
      "第124次爬取\n",
      "------爬取成功！-----\n",
      "第125次爬取\n",
      "------爬取成功！-----\n",
      "第126次爬取\n",
      "------爬取成功！-----\n",
      "第127次爬取\n",
      "------爬取成功！-----\n",
      "第128次爬取\n",
      "------爬取成功！-----\n",
      "第129次爬取\n",
      "------爬取成功！-----\n",
      "第130次爬取\n",
      "------爬取成功！-----\n",
      "第131次爬取\n",
      "------爬取成功！-----\n",
      "第132次爬取\n",
      "------爬取成功！-----\n",
      "第133次爬取\n",
      "------爬取成功！-----\n",
      "第134次爬取\n",
      "------爬取成功！-----\n",
      "第135次爬取\n",
      "------爬取成功！-----\n",
      "第136次爬取\n",
      "------爬取成功！-----\n",
      "第137次爬取\n",
      "------爬取成功！-----\n",
      "第138次爬取\n",
      "------爬取成功！-----\n",
      "第139次爬取\n",
      "------爬取成功！-----\n",
      "第140次爬取\n",
      "------爬取成功！-----\n",
      "第141次爬取\n",
      "------爬取成功！-----\n",
      "第142次爬取\n",
      "------爬取成功！-----\n",
      "第143次爬取\n",
      "------爬取成功！-----\n",
      "第144次爬取\n",
      "------爬取成功！-----\n",
      "第145次爬取\n",
      "------爬取成功！-----\n",
      "第146次爬取\n",
      "------爬取成功！-----\n",
      "第147次爬取\n",
      "------爬取成功！-----\n",
      "第148次爬取\n",
      "------爬取成功！-----\n",
      "第149次爬取\n",
      "------爬取成功！-----\n",
      "第150次爬取\n",
      "------爬取成功！-----\n",
      "第151次爬取\n",
      "------爬取成功！-----\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ure\n",
    "import urllib.error\n",
    "import re\n",
    "\n",
    "url = 'http://blog.csdn.net'\n",
    "\n",
    "hearders = ('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36')\n",
    "\n",
    "data = ure.urlopen(url).read()\n",
    "data = data.decode('utf-8')\n",
    "pat = 'href=\"(https://blog.csdn.net/.*?)\"'\n",
    "all_url = re.compile(pat).findall(data)\n",
    "for i in range(len(all_url)):\n",
    "    try:\n",
    "        print('第'+str(i)+'次爬取')\n",
    "        this_url = all_url[i]\n",
    "        file = './csdnblog/'+str(i)+'.html'\n",
    "        ure.urlretrieve(this_url, file)\n",
    "        print('------爬取成功！-----')\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, 'code'):\n",
    "            print(e.code)\n",
    "        if hasattr(e, '%reason'):\n",
    "            print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫防屏蔽手段之代理服务器实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是代理服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用代理服务器进行爬去网页实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "622px",
    "left": "92px",
    "right": "20px",
    "top": "142px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
